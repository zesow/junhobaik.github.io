---
tags:
  - docker
  - dockerfile
---



## 목표

 크롤링 속도를 향상시키기 위해서는 병렬처리로 해야되는데, docker container를 여러 대 실행해서 각각에게 몇 번 페이지부터 몇 번 페이지까지 파라미터로 주어 크롤링하는 방식으로 시도해 본다. 그러기 위해서 크롤링 환경이 갖추어진 docker 이미지를 만드는 과정이 필요하다.



## 조건

 내가 만들 이미지는 다음과 같은 기능을 갖추어야 한다.

1. 크롤링 할 페이지 수를 파라미터로 받을 수 있어야 함. 파라미터는 시작 페이지, 끝 페이지 2개가 필요함.
2. 1번 파라미터가 컨테이너 내 크롤링 파이썬 코드 실행 시 파라미터로 들어가서 실행됨.
3. 크롤링을 하기 위한 모든 환경이 이미지에 들어가 있어야 함. 내 크롤링 코드에서 사용하는 조건은 다음과 같음.
   1. python3
   2. beautifulsoup4
   3. urllib
4. 크롤링이 완료 되서 나온 json 파일은 host의 원하는 폴더에 전송되야 함.



## 파이썬 코드 수정하기

 시작과 끝 페이지 수를 파라미터로 받도록 코드를 수정함.

```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
import json
from collections import OrderedDict
from urllib.error import URLError
import sys

urlbefore = "주소주소"
urlafter = "주소주소"

jsonList = []
folder_name = 'test'

sPage = int(sys.argv[1])
ePage = int(sys.argv[2])

page = sPage

while page <= ePage:
    url = urlbefore + str(page) + urlafter
    try:
        html = urlopen(url)
    except URLError as e:
        print(e)
        continue

    bsObj = BeautifulSoup(html,"html.parser")

    rObject = bsObj.findAll("li",{"class":"reviews_index_list_review"})

    for obj in rObject:
        options = obj.find("div",{"class":"review_options"})

        reviews = obj.find("div",{"class":"reviews_index_list_review__message_expanded"})
        reviewers = obj.find("span",{"class":"reviews_index_list_review__name"})
        date = obj.find("span",{"class":"reviews_index_list_review__date"})
        evals = obj.find("span",{"class":"reviews_index_list_review__rating_item reviews_index_list_review__text_rating"})
        products = obj.find("a",{"class":"reviews_index_list_review__title_text js-link-iframe"})
        images = obj.findAll("img",{"class":"js-review-image smooth"})
        helpfuls_plus = obj.find("strong",{"class":"js-like-score-plus"})
        helpfuls_total = obj.find("strong",{"class":"js-like-score-total"})
        review_json = OrderedDict()
        review_json['pTitle'] = products.get_text().strip()
        review_json['cId'] = reviewers.get_text().strip()
        review_json['rNo'] = int(obj.get('id').split('_')[1])
        review_json['_id'] = 'artlance_'+str(review_json['rNo'])
        review_json['rDate'] = date.get_text().strip()
        review_json['helpful'] = [int(helpfuls_plus.get_text().strip()),int(helpfuls_total.get_text().strip())]
        rScore_temp = evals.get_text().replace('-','').strip()
        if(rScore_temp == "아주 좋아요"):
            review_json['rScore'] = 5.0
        elif(rScore_temp == "맘에 들어요"):
            review_json['rScore'] = 4.0
        elif(rScore_temp == "보통이에요"):
            review_json['rScore'] = 3.0
        elif(rScore_temp == "그냥 그래요"):
            review_json['rScore'] = 2.0
        else:
            review_json['rScore'] = 1.0

        if options is not None:
            titles = options.findAll("div",{"class":"review_option__title"})
            contents = options.findAll("div",{"class":"review_option__content"})


            for j in range(len(titles)):
                if(titles[j].get_text() == "선택한 옵션"):
                    continue

                if(titles[j].get_text() == '키'):
                    review_json['height'] = int(contents[j].get_text().split(' ')[0])
                elif(titles[j].get_text() == '몸무게'):
                    review_json['weight'] = contents[j].get_text()
                elif(titles[j].get_text() == '평소사이즈'):
                    try:
                        review_json['bodySize'] = int(contents[j].get_text())
                    except ValueError:
                        review_json['bodySize'] = contents[j].get_text()


            chooses_titles = options.findAll("span",{"class":"review_option__product_option_key"})
            chooses_contents = options.findAll("span",{"class":"review_option__product_option_value"})
            for j in range(len(chooses_titles)):
                if(chooses_titles[j].get_text() == '색상:'):
                    review_json['pColor'] = chooses_contents[j].get_text()
                else:
                    review_json['pSize'] = chooses_contents[j].get_text()
        review_json['pURL'] = products.get('data-url')
        review_json['pID'] = int(review_json['pURL'].split('index_no=')[1])

        if len(images) is not 0:
            imgList=[]
            for j in images:
                imgList.append(j.get('src').replace("//",""))
            review_json['rImages'] = imgList



        review_json['desc'] = reviews.get_text().strip()

        json_now = json.dumps(review_json, ensure_ascii=True, indent="\t")
        #print(json_now)
        jsonList.append(review_json)

    # print(jsonList)

    page = page + 1


fname = '/data/file_'+str(sPage)+'-'+str(ePage)+'.json'
with open(fname, 'w') as fp:
    json.dump(jsonList, fp, ensure_ascii=True, indent="\t")
```





## 마지막 페이지 알아내는 코드 작성(이건 다음 챕터에서 쓸 것)

container 들에게 page를 분배하기 위해서 총 페이지를 알아낸다.

이진 탐색 방법으로 수동 구현하였더니 약 10초 정도 걸리는 것 같다. 

```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
import json
from collections import OrderedDict
from urllib.error import URLError
import sys



urlbefore = "주소주소"
urlafter = "주소주소"

sPage = 1
ePage = 100000
mPage = 0


while sPage <= ePage:
    mPage = (sPage + ePage) / 2
    
    url = urlbefore + str(mPage) + urlafter
    try:
        html = urlopen(url)
    except URLError as e:
        print(e)
        continue
    
    bsObj = BeautifulSoup(html,"html.parser")

    rObject = bsObj.findAll("li",{"class":"reviews_index_list_review"})

    if len(rObject) != 0:
        sPage = mPage + 1
    else:
        ePage = mPage - 1
print(int(mPage))
```





## ubuntu official image 에서 시작해보기

 공식 맨 ubuntu 만 run 한 container 에서,

1. 깔아야 할 라이브러리 설치하기
2. 위에서 수정한 파이썬 코드 잘 작동 하는지 확인하기
3. 결과물로 나온 json 파일 host로 전송하기

가 되도록 환경 설정함.

### 필요한 것

#### 업데이트

`apt-get update`



#### 파이썬 설치

`apt-get install -y python3`



#### beautifulsoup 4 설치

`apt-get install -y python3-bs4`



#### run 시 host의 볼륨을 지정해 줘서 그곳에 json 파일이 모이게 하기

`docker run -it --name crawling1 -v /tmp/data:/data d086b06dcc59 /bin/bash`

놀랍게도 host의 /tmp/data에 파일이 모인다 !!

실제 container 내에서 저장되는 경로는 /data 에 데이터를 넣어줘야 함. 마치 포트 맞추듯이.

[볼륨 관련 참조링크1](http://pyrasis.com/book/DockerForTheReallyImpatient/Chapter06/04)

[볼륨 관련 참조링크2](https://darkrasid.github.io/docker/container/volume/2017/05/10/docker-volumes.html)



#### docker run 시 페이지 번호 파라미터로 주기

host에서 준 페이지 시작과 끝 파라미터가 python 까지 흘러들어가도록.

shell script로 crawling 하는 파이썬 코드를 실행하면 됨.

```shell
#!/bin/sh

`echo python3 crawling.py $1 $2`
```









## dockerfile 생성하기

 위에서 설정한 환경 대로 dockerfile 을 작성해 줌.

```dockerfile
FROM ubuntu:latest

WORKDIR /app

ADD . /app

RUN apt-get update
RUN apt-get install -y python3
RUN apt-get install -y python3-bs4

ENTRYPOINT ["bash","entrypoint.sh"]
```



### build 하기

`docker build -t my_crawler_0.1 .`

### run 하기

`docker run -v /tmp/data:/data my_crawler_0.1 1 3`





## 다음에 할 일

이번 포스팅에서는 원하는 페이지를 크롤링해서 원하는 곳에 저장하는 이미지를 만들었다. 

이제 만들었으니 쓸 차례이다.



### 병렬로 동시에 실행할 수 있는 스크립트 작성하기

 스크립트는 다음과 같은 기능을 수행해야 함.

1. 해당 쇼핑몰의 총 페이지 수를 계산해야 함.
2. 그 페이지 수를 N 개의 container 에게 N등분 하여 분배해야 함.
3. N개의 container를 run 해야 함.